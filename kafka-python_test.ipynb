{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7851242-78c9-417e-91a5-37093f83d5df",
   "metadata": {},
   "source": [
    "## Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "733355fb-2d8a-427a-84cf-fbd98f040f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl.metadata (7.8 kB)\n",
      "Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "   ---------------------------------------- 0.0/246.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/246.5 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/246.5 kB 660.6 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 204.8/246.5 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 246.5/246.5 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a13bf9-8824-4294-a4c1-ae987ce74f33",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load important libraries\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd2c77a-5971-467c-95a4-41dff2796949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create producer\n",
    "producer = KafkaProducer(bootstrap_servers=['172.29.16.101:9092'],\n",
    "                         value_serializer=lambda x: dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8c031-a427-493c-b6cc-436709f409ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### define topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e50f36e4-f642-4f37-a061-2b5bade99c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic = 'traffic_data_group7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee371e69-f591-4b56-b8aa-c8f5ea9858c6",
   "metadata": {},
   "source": [
    "### send message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02e2c5ec-3547-4ec1-b42c-e5a330b7a171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x18611fe7ad0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now write a message to an existing topic\n",
    "# (with a delay of 5 seconds)\n",
    "data = {'test' : \"traffic data test\"}\n",
    "producer.send(topic, value=data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7faaf-9967-4a66-a781-3a404e295d1b",
   "metadata": {},
   "source": [
    "## Current Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6314c-684a-476c-a104-f0d258c8eb13",
   "metadata": {},
   "source": [
    "Kafka topics act as channels for messages in an event streaming platform. Producers send data streams to specific topics, and consumers subscribe to those topics to receive the data. Imagine them as folders for categorized messages in a constantly flowing river of information. ou can write Python code using the `confluent_kafka.admin` library to programmatically list topics. This approach is useful if you want to integrate topic listing within your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c957e7a1-1279-4bcf-82c6-44ed1f2f905c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c5cf80-9436-4c7f-a147-bb02c6f3fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': '172.29.16.101:9092'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab137c0a-5eda-4d3a-b75f-64e77a56063a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kadmin = AdminClient(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c01a9f7-9a38-41c6-a41a-12f26fc04415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'here-api-routes': TopicMetadata(here-api-routes, 1 partitions),\n",
       " 'traffic_data_group7': TopicMetadata(traffic_data_group7, 1 partitions),\n",
       " 'stock-prices': TopicMetadata(stock-prices, 1 partitions),\n",
       " 'hello-world': TopicMetadata(hello-world, 1 partitions),\n",
       " 'london_routes': TopicMetadata(london_routes, 1 partitions),\n",
       " 'yahoo-finance': TopicMetadata(yahoo-finance, 1 partitions),\n",
       " 'wikimedia-changes': TopicMetadata(wikimedia-changes, 1 partitions),\n",
       " 'delhi_routes': TopicMetadata(delhi_routes, 1 partitions),\n",
       " 'testTopic': TopicMetadata(testTopic, 1 partitions),\n",
       " 'roulette': TopicMetadata(roulette, 1 partitions),\n",
       " 'berlin_routes': TopicMetadata(berlin_routes, 1 partitions),\n",
       " 'here_api_data': TopicMetadata(here_api_data, 1 partitions),\n",
       " 'traffic-data': TopicMetadata(traffic-data, 1 partitions),\n",
       " 'hello': TopicMetadata(hello, 1 partitions),\n",
       " 'nyt_article_publishes': TopicMetadata(nyt_article_publishes, 1 partitions),\n",
       " '__consumer_offsets': TopicMetadata(__consumer_offsets, 50 partitions)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kadmin.list_topics().topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bf8c8be-220b-42a8-a90e-20a9598f4e82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Future at 0x269bc6c9f50 state=running>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kadmin.list_consumer_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf9847-8306-4278-ad3f-a1b3a2cd8c4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kafka Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98de96-df9b-4fd9-aa20-2eeccefacd1d",
   "metadata": {},
   "source": [
    "A Kafka consumer is a program that listens for and processes messages from specific Kafka topics. Think of it as an ear in a bustling marketplace, tuned in to receive messages of interest published by producers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb23dafd-c88f-49ad-84a2-490a89947bb9",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    " bootstrap_servers='172.29.16.101:9092',\n",
    " value_deserializer = lambda v: json.loads(v.decode('ascii')),\n",
    " auto_offset_reset='earliest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab0dd8-c440-4756-9fd2-4ae2923623d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:7: v=traffic data test\n"
     ]
    }
   ],
   "source": [
    "consumer.subscribe(topics=topic)\n",
    "for m in consumer:\n",
    "  print (\"%d:%d: v=%s\" % (m.partition,\n",
    "                          m.offset,\n",
    "                          m.value['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "201b95de-c6d3-4f97-bf31-0cf95b4591fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0: v=OK\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m consumer\u001b[38;5;241m.\u001b[39msubscribe(topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnyt_article_publishes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: v=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (m\u001b[38;5;241m.\u001b[39mpartition,\n\u001b[0;32m      4\u001b[0m                           m\u001b[38;5;241m.\u001b[39moffset,\n\u001b[0;32m      5\u001b[0m                           m\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v2()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1115\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m-> 1116\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39mtimeout_ms, update_offsets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[0;32m   1122\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    653\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_once(remaining, max_records, update_offsets\u001b[38;5;241m=\u001b[39mupdate_offsets)\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[0;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\consumer\\group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    701\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m--> 702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39mtimeout_ms)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    599\u001b[0m             timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    600\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[0;32m    606\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kafka\\client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[0;32m    633\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 634\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[0;32m    635\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\selectors.py:323\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    321\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writers, [], timeout)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\selectors.py:314\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 314\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m select\u001b[38;5;241m.\u001b[39mselect(r, w, w, timeout)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "consumer.subscribe(topics='nyt_article_publishes')\n",
    "for m in consumer:\n",
    "  print (\"%d:%d: v=%s\" % (m.partition,\n",
    "                          m.offset,\n",
    "                          m.value['status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d25270-1d8b-4aaa-a6f7-905abcba2f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
